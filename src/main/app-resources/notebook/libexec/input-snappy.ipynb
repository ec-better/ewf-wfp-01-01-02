{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## WFP-01-01-02 Sentinel-1 coherence timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This application takes a pair of Sentinel-1 products and identifies and generates the coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <a name=\"quicklink\">Quick link\n",
    "\n",
    "* [Objective](#objective)\n",
    "* [Test Site](#test-site)\n",
    "* [Context](#context)\n",
    "* [Applicability](#applicability)\n",
    "* [Data](#data)\n",
    "* [Service Definition](#service)\n",
    "* [Parameter Definition](#parameter)\n",
    "* [Runtime Parameter Definition](#runtime)\n",
    "* [Workflow](#workflow)\n",
    "* [Strengths and Limitations](#strengths-limitations) \n",
    "* [License](#license)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As a data processor developer, I want to implement, and package an algorithm processing a pair of S1 SAR SLC datasets using the SNAP toolbox notebook archetype with the following processing steps:\n",
    "\n",
    "A. S1 SAR processing per image:\n",
    "\n",
    "* Application of orbit file (should wait for the orbit file a couple of days, since for coherence this is important)\n",
    "* TOPS slice assembly (if necessary)\n",
    "* TOPS split (if necessary)\n",
    "\n",
    "B. For image pair:\n",
    "\n",
    "* TOPS coregistration\n",
    "* Coherence estimation (Given set of images from same orbit,  {t1, t2, t3, ... , tn}, two temporally adjacent images would * constitute an image pair, with the first one being the master. So the pairs would be: {t1 - t2, t2 - t3, ... , tn-1 - tn}.)\n",
    "* TOPS deburst\n",
    "* Multi-looking\n",
    "* Terrain correction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"service\">Service definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = dict([('title', 'WFP-01-01-02 Sentinel-1 coherence'),\n",
    "                ('abstract', 'WFP-01-01-02 Sentinel-1 coherence'),\n",
    "                ('id', 'ewf-wfp-01-01-02')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "swaths = dict([('id', 'swaths'),\n",
    "               ('value', 'IW1,IW2,IW3'),\n",
    "               ('title', 'Sentinel-1 sub-swaths'),\n",
    "               ('abstract', 'Sentinel-1 sub-swaths')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subswaths = list(swaths['value'].split(',')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarisation = dict([('id', 'polarisation'),\n",
    "                     ('value', 'VH'),\n",
    "                     ('title', 'Selected Polarisation'),\n",
    "                     ('abstract', 'Choose ''VV''or ''VH'' or ''VV,VH'' to include all Polarisations')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "polarisations = list(polarisation['value'].split(',')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(polarisations)==2:\n",
    "    selected_Pol=False \n",
    "else:\n",
    "    selected_Pol=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"runtime\">Runtime parameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input identifiers**\n",
    "\n",
    "These are the Sentinel-1 product identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_identifiers = ('S1A_IW_SLC__1SDV_20170105T035027_20170105T035041_014691_017E74_5F66',\n",
    "                     'S1A_IW_SLC__1SDV_20170105T035000_20170105T035030_014691_017E74_0110',\n",
    "                     'S1A_IW_SLC__1SDV_20170117T035008_20170117T035037_014866_0183DD_C2F1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input references**\n",
    "\n",
    "These are the Sentinel-1 catalogue references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_references = ('https://catalog.terradue.com/sentinel1/search?format=atom&uid=S1A_IW_SLC__1SDV_20170105T035027_20170105T035041_014691_017E74_5F66',\n",
    "                    'https://catalog.terradue.com/sentinel1/search?format=atom&uid=S1A_IW_SLC__1SDV_20170105T035000_20170105T035030_014691_017E74_0110',\n",
    "                    'https://catalog.terradue.com/sentinel1/search?format=atom&uid=S1A_IW_SLC__1SDV_20170117T035008_20170117T035037_014866_0183DD_C2F1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Data path**\n",
    "\n",
    "This path defines where the data is staged-in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '/workspace/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"workflow\">Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the packages required for processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from snappy import jpy\n",
    "from snappy import ProductIO\n",
    "from snappy import GPF\n",
    "from snappy import HashMap\n",
    "\n",
    "import dateutil.parser as parser\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "import gdal\n",
    "import osr\n",
    "\n",
    "import lxml.etree as etree\n",
    "\n",
    "from shapely.wkt import loads\n",
    "import numpy as np\n",
    "\n",
    "from shapely.geometry import box\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "sys.path.append('/opt/anaconda/bin/')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from snappy import jpy\n",
    "from snappy import ProductIO\n",
    "from snappy import GPF\n",
    "from snappy import HashMap\n",
    "\n",
    "import gc\n",
    "import cioppy\n",
    "\n",
    "import lxml.etree as etree\n",
    "\n",
    "from shapely.wkt import loads\n",
    "\n",
    "import gdal\n",
    "import osr\n",
    "\n",
    "from shapely.geometry import box\n",
    "\n",
    "import gzip\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_identifiers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6e58cbbc6e42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midentifier\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_identifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0ms1_zip_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midentifier\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_identifiers' is not defined"
     ]
    }
   ],
   "source": [
    "s1meta = \"manifest.safe\"\n",
    "\n",
    "slave_date = ''\n",
    "\n",
    "slave_products = []\n",
    "master_products = []\n",
    "\n",
    "slave_prefix = []\n",
    "master_prefix = []\n",
    "\n",
    "dates = []\n",
    "\n",
    "for index, identifier in enumerate(input_identifiers):\n",
    "\n",
    "    s1_zip_file = os.path.join(data_path, identifier + '.zip') \n",
    "    s1_meta_file = os.path.join(data_path, identifier, identifier + '.SAFE', 'manifest.safe') \n",
    "\n",
    "    if os.path.isfile(s1_zip_file):\n",
    "        s1prd = s1_zip_file\n",
    "    elif os.path.isfile(s1_meta_file):\n",
    "        s1prd = s1_meta_file\n",
    "\n",
    "    reader = ProductIO.getProductReader(\"SENTINEL-1\")\n",
    "    product = reader.readProductNodes(s1prd, None)\n",
    "    \n",
    "    \n",
    "    width = product.getSceneRasterWidth()\n",
    "    height = product.getSceneRasterHeight()\n",
    "    name = product.getName()\n",
    "    start_date = parser.parse(product.getStartTime().toString()).isoformat()\n",
    "    \n",
    "    dates.append(start_date[:19])\n",
    "    if index == 0:\n",
    "        \n",
    "        slave_date = start_date[:10]\n",
    "        slave_products.append(product)\n",
    "        print(\"Product: %s, %d x %d pixels of %s assigned as slave\" % (name, width, height, start_date))\n",
    "        \n",
    "        slave_data_take = identifier.split('_')[-2]  \n",
    "        slave_prefix.append(identifier.split('_')[-1]) \n",
    "    else:\n",
    "        \n",
    "        if start_date[:10] == slave_date:\n",
    "            slave_products.append(product)\n",
    "            print(\"Product: %s, %d x %d pixels of %s assigned as slave\" % (name, width, height, start_date))\n",
    "            slave_prefix.append(identifier.split('_')[-1]) \n",
    "        else:\n",
    "            master_products.append(product)\n",
    "            print(\"Product: %s, %d x %d pixels of %s assigned as master\" % (name, width, height, start_date))\n",
    "            master_data_take = identifier.split('_')[-2]  \n",
    "            master_prefix.append(identifier.split('_')[-1]) \n",
    "\n",
    "            \n",
    "            \n",
    "output_name = 'S1_COH_%s_%s_%s_%s_%s_%s_%s_%s_%s' % (parser.parse(min(dates)).strftime('%Y%m%d%H%M%S'),\n",
    "                                                     parser.parse(max(dates)).strftime('%Y%m%d%H%M%S'),\n",
    "                                                     slave_data_take,\n",
    "                                                     len(input_identifiers)/2, \n",
    "                                                     '_'.join(slave_prefix),\n",
    "                                                     master_data_take,\n",
    "                                                     len(input_identifiers)/2, \n",
    "                                                     '_'.join(master_prefix),\n",
    "                                                    polarisation['value'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S1_COH_20170105035000_20170117035008_017E74_1_5F66_0110_0183DD_1_C2F1'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble the slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('selectedPolarisations', None)\n"
     ]
    }
   ],
   "source": [
    "operator = 'SliceAssembly'\n",
    "\n",
    "op_spi = GPF.getDefaultInstance().getOperatorSpiRegistry().getOperatorSpi(operator)\n",
    "\n",
    "op_params = op_spi.getOperatorDescriptor().getParameterDescriptors()\n",
    "\n",
    "for param in op_params:\n",
    "    print(param.getName(), param.getDefaultValue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(slave_products) > 1:\n",
    "    parameters = HashMap()\n",
    "    if selected_Pol:\n",
    "        parameters.put('selectedPolarisations', polarisation['value'])\n",
    "    else:\n",
    "        parameters.put('selectedPolarisations', None)\n",
    "    slave_slice = GPF.createProduct(operator,parameters, slave_products)\n",
    "\n",
    "else:\n",
    "    slave_slice = slave_products[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(master_products) >1:\n",
    "\n",
    "    parameters = HashMap()\n",
    "    if selected_Pol:\n",
    "        parameters.put('selectedPolarisations', polarisation['value'])\n",
    "    else:\n",
    "        parameters.put('selectedPolarisations', None)\n",
    "    \n",
    "\n",
    "    master_slice = GPF.createProduct(operator,\n",
    "                       parameters, \n",
    "                       master_products)\n",
    "\n",
    "else: \n",
    "    master_slice = master_products[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply orbit file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('orbitType', 'Sentinel Precise (Auto Download)')\n",
      "('polyDegree', '3')\n",
      "('continueOnFail', 'false')\n"
     ]
    }
   ],
   "source": [
    "operator = 'Apply-Orbit-File'\n",
    "\n",
    "op_spi = GPF.getDefaultInstance().getOperatorSpiRegistry().getOperatorSpi(operator)\n",
    "\n",
    "op_params = op_spi.getOperatorDescriptor().getParameterDescriptors()\n",
    "\n",
    "for param in op_params:\n",
    "    print(param.getName(), param.getDefaultValue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_orbit_products = []\n",
    "slave_orbit_products = []\n",
    "\n",
    "HashMap = jpy.get_type('java.util.HashMap')\n",
    "GPF.getDefaultInstance().getOperatorSpiRegistry().loadOperatorSpis()\n",
    "\n",
    "parameters = HashMap()\n",
    "\n",
    "parameters.put('orbitType', 'Sentinel Precise (Auto Download)')\n",
    "parameters.put('polyDegree', '3')\n",
    "parameters.put('continueOnFail', 'false')\n",
    "\n",
    "\n",
    "master_orbit = GPF.createProduct(operator,\n",
    "                                 parameters, \n",
    "                                 master_slice)\n",
    "    \n",
    "\n",
    "slave_orbit= GPF.createProduct(operator,\n",
    "                               parameters, \n",
    "                               slave_slice)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('subswath', None)\n",
      "('selectedPolarisations', None)\n",
      "('firstBurstIndex', '1')\n",
      "('lastBurstIndex', '9999')\n",
      "('wktAoi', None)\n"
     ]
    }
   ],
   "source": [
    "operator = 'TOPSAR-Split'\n",
    "\n",
    "op_spi = GPF.getDefaultInstance().getOperatorSpiRegistry().getOperatorSpi(operator)\n",
    "\n",
    "op_params = op_spi.getOperatorDescriptor().getParameterDescriptors()\n",
    "\n",
    "for param in op_params:\n",
    "    print(param.getName(), param.getDefaultValue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_split_prds = []\n",
    "slave_split_prds= []\n",
    "\n",
    "for subswath in subswaths:  \n",
    "    \n",
    "    parameters = HashMap()\n",
    "\n",
    "    parameters.put('subswath', subswath)\n",
    "    if selected_Pol:\n",
    "        parameters.put('selectedPolarisations', polarisation['value'])\n",
    "    else:\n",
    "        parameters.put('selectedPolarisations', None)\n",
    "    \n",
    "    parameters.put('firstBurstIndex', '1')\n",
    "    parameters.put('lastBurstIndex', '9999')\n",
    "\n",
    "    master_split_prds.append(GPF.createProduct(operator,\n",
    "                           parameters, \n",
    "                           master_orbit))   \n",
    "    \n",
    "    slave_split_prds.append(GPF.createProduct(operator,\n",
    "                           parameters, \n",
    "                           slave_orbit))   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master_orbit = None\n",
    "#slave_orbit = None\n",
    "#gc.collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-Geocoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('demName', 'SRTM 3Sec')\n",
      "('demResamplingMethod', 'BICUBIC_INTERPOLATION')\n",
      "('externalDEMFile', None)\n",
      "('externalDEMNoDataValue', '0')\n",
      "('resamplingType', 'BISINC_5_POINT_INTERPOLATION')\n",
      "('maskOutAreaWithoutElevation', 'true')\n",
      "('outputRangeAzimuthOffset', 'false')\n",
      "('outputDerampDemodPhase', 'false')\n",
      "('disableReramp', 'false')\n"
     ]
    }
   ],
   "source": [
    "operator = 'Back-Geocoding'\n",
    "\n",
    "op_spi = GPF.getDefaultInstance().getOperatorSpiRegistry().getOperatorSpi(operator)\n",
    "\n",
    "op_params = op_spi.getOperatorDescriptor().getParameterDescriptors()\n",
    "\n",
    "for param in op_params:\n",
    "    print(param.getName(), param.getDefaultValue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "backgeo_prds = []\n",
    "\n",
    "for index, subswath in enumerate(subswaths):  \n",
    "    \n",
    "\n",
    "    parameters = HashMap()\n",
    "\n",
    "    for param in op_params:\n",
    "        parameters.put(param.getName(), param.getDefaultValue())\n",
    "\n",
    "\n",
    "    backgeo_prds.append(GPF.createProduct(operator,\n",
    "                           parameters, \n",
    "                           [master_split_prds[index], \n",
    "                            slave_split_prds[index]\n",
    "                           ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cohWinAz', '10')\n",
      "('cohWinRg', '10')\n",
      "('subtractFlatEarthPhase', 'false')\n",
      "('srpPolynomialDegree', '5')\n",
      "('srpNumberPoints', '501')\n",
      "('orbitDegree', '3')\n",
      "('squarePixel', 'true')\n",
      "('subtractTopographicPhase', 'false')\n",
      "('demName', 'SRTM 3Sec')\n",
      "('externalDEMFile', None)\n",
      "('externalDEMNoDataValue', '0')\n",
      "('externalDEMApplyEGM', 'true')\n",
      "('tileExtensionPercent', '100')\n"
     ]
    }
   ],
   "source": [
    "operator = 'Coherence'\n",
    "\n",
    "op_spi = GPF.getDefaultInstance().getOperatorSpiRegistry().getOperatorSpi(operator)\n",
    "\n",
    "op_params = op_spi.getOperatorDescriptor().getParameterDescriptors()\n",
    "\n",
    "parameters = HashMap()\n",
    "\n",
    "for param in op_params:\n",
    "    print(param.getName(), param.getDefaultValue())\n",
    "    parameters.put(param.getName(), param.getDefaultValue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_prds = []\n",
    "\n",
    "for index, subswath in enumerate(subswaths):  \n",
    "    \n",
    "\n",
    "    parameters = HashMap()\n",
    "\n",
    "    for param in op_params:\n",
    "        parameters.put(param.getName(), param.getDefaultValue())\n",
    "\n",
    "    coherence_prds.append(GPF.createProduct(operator,\n",
    "                           parameters, \n",
    "                           backgeo_prds[index]))\n",
    "\n",
    "\n",
    "\n",
    "#back_geocoding_iw1 = None\n",
    "#back_geocoding_iw2 = None\n",
    "#back_geocoding_iw3 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('selectedPolarisations', None)\n"
     ]
    }
   ],
   "source": [
    "operator = 'TOPSAR-Deburst'\n",
    "\n",
    "op_spi = GPF.getDefaultInstance().getOperatorSpiRegistry().getOperatorSpi(operator)\n",
    "\n",
    "op_params = op_spi.getOperatorDescriptor().getParameterDescriptors()\n",
    "\n",
    "for param in op_params:\n",
    "    print(param.getName(), param.getDefaultValue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "deburst_prds = []\n",
    "\n",
    "for index, subswath in enumerate(subswaths):  \n",
    " \n",
    "    parameters = HashMap()\n",
    "    if selected_Pol:\n",
    "        parameters.put('selectedPolarisations', polarisation['value'])\n",
    "    else:\n",
    "        parameters.put('selectedPolarisations', None)\n",
    "    \n",
    "\n",
    "    deburst_prds.append(GPF.createProduct(operator,\n",
    "                           parameters, \n",
    "                           coherence_prds[index]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coherence_iw1 = None\n",
    "#coherence_iw2 = None\n",
    "#coherence_iw3 = None\n",
    "\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('selectedPolarisations', None)\n"
     ]
    }
   ],
   "source": [
    "operator = 'TOPSAR-Merge'\n",
    "\n",
    "op_spi = GPF.getDefaultInstance().getOperatorSpiRegistry().getOperatorSpi(operator)\n",
    "\n",
    "op_params = op_spi.getOperatorDescriptor().getParameterDescriptors()\n",
    "\n",
    "for param in op_params:\n",
    "    print(param.getName(), param.getDefaultValue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = HashMap()\n",
    "if selected_Pol:\n",
    "        parameters.put('selectedPolarisations', polarisation['value'])\n",
    "else:\n",
    "        parameters.put('selectedPolarisations', None)\n",
    "\n",
    "\n",
    "tops_merge = GPF.createProduct(operator,\n",
    "                           parameters, \n",
    "                           deburst_prds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deburst_iw1 = None\n",
    "#deburst_iw2 = None\n",
    "#deburst_iw3 = None\n",
    "\n",
    "#gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "band_name = list(tops_merge.getBandNames())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = HashMap()\n",
    "\n",
    "parameters.put('sourceBands', band_name)\n",
    "parameters.put('demName', 'SRTM 1Sec HGT')\n",
    "parameters.put('externalDEMFile', '')\n",
    "parameters.put('externalDEMNoDataValue', '0.0')\n",
    "parameters.put('externalDEMApplyEGM', 'true')\n",
    "parameters.put('demResamplingMethod', 'BILINEAR_INTERPOLATION')\n",
    "parameters.put('imgResamplingMethod', 'BILINEAR_INTERPOLATION')\n",
    "parameters.put('pixelSpacingInMeter', '10.0')\n",
    "#parameters.put('pixelSpacingInDegree', '8.983152841195215E-5')\n",
    "parameters.put('mapProjection', 'AUTO:42001')\n",
    "parameters.put('nodataValueAtSea', 'true')\n",
    "parameters.put('saveDEM', 'false')\n",
    "parameters.put('saveLatLon', 'false')\n",
    "parameters.put('saveIncidenceAngleFromEllipsoid', 'false')\n",
    "parameters.put('saveProjectedLocalIncidenceAngle', 'false')\n",
    "parameters.put('saveSelectedSourceBand', 'true')\n",
    "parameters.put('outputComplex', 'false')\n",
    "parameters.put('applyRadiometricNormalization', 'false')\n",
    "parameters.put('saveSigmaNought', 'false')\n",
    "parameters.put('saveGammaNought', 'false')\n",
    "parameters.put('saveBetaNought', 'false')\n",
    "parameters.put('incidenceAngleForSigma0', 'Use projected local incidence angle from DEM')\n",
    "parameters.put('incidenceAngleForGamma0', 'Use projected local incidence angle from DEM')\n",
    "parameters.put('auxFile', 'Latest Auxiliary File')\n",
    "\n",
    "terrain_correction = GPF.createProduct('Terrain-Correction', parameters, tops_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tops_merge = None\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#terrain_correction = None\n",
    "#gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProductIO.writeProduct(terrain_correction, output_name + '.tif', 'GeoTIFF-BigTIFF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eop_metadata(metadata):\n",
    "\n",
    "    opt = 'http://www.opengis.net/opt/2.1'\n",
    "    om  = 'http://www.opengis.net/om/2.0'\n",
    "    gml = 'http://www.opengis.net/gml/3.2'\n",
    "    eop = 'http://www.opengis.net/eop/2.1'\n",
    "    sar = 'http://www.opengis.net/sar/2.1'\n",
    "    \n",
    "    root = etree.Element('{%s}EarthObservation' % sar)\n",
    "\n",
    "    phenomenon_time = etree.SubElement(root, '{%s}phenomenonTime' % om)\n",
    "\n",
    "    time_period = etree.SubElement(phenomenon_time, '{%s}TimePeriod' % gml)\n",
    "\n",
    "    begin_position = etree.SubElement(time_period, '{%s}beginPosition'  % gml)\n",
    "\n",
    "    end_position = etree.SubElement(time_period, '{%s}endPosition'  % gml)\n",
    "\n",
    "    procedure = etree.SubElement(root, '{%s}procedure' % om)\n",
    "\n",
    "    earth_observation_equipment = etree.SubElement(procedure, '{%s}EarthObservationEquipment' % eop)\n",
    "\n",
    "    acquisition_parameters = etree.SubElement(earth_observation_equipment, '{%s}acquisitionParameters' % eop)\n",
    "\n",
    "    acquisition = etree.SubElement(acquisition_parameters, '{%s}Acquisition' % sar)\n",
    "\n",
    "    orbit_number = etree.SubElement(acquisition, '{%s}orbitNumber' % eop)\n",
    "\n",
    "    wrs_longitude_grid = etree.SubElement(acquisition, '{%s}wrsLongitudeGrid' % eop)\n",
    "\n",
    "    polarisation_channels = etree.SubElement(acquisition, '{%s}polarisationChannels' % eop)\n",
    "    \n",
    "    feature_of_interest = etree.SubElement(root, '{%s}featureOfInterest' % om)\n",
    "    footprint = etree.SubElement(feature_of_interest, '{%s}Footprint' % eop)\n",
    "    multi_extentOf = etree.SubElement(footprint, '{%s}multiExtentOf' % eop)\n",
    "    multi_surface = etree.SubElement(multi_extentOf, '{%s}MultiSurface' % gml)\n",
    "    surface_members = etree.SubElement(multi_surface, '{%s}surfaceMembers' % gml)\n",
    "    polygon = etree.SubElement(surface_members, '{%s}Polygon' % gml)    \n",
    "    exterior = etree.SubElement(polygon, '{%s}exterior' % gml)  \n",
    "    linear_ring = etree.SubElement(exterior, '{%s}LinearRing' % gml) \n",
    "    poslist = etree.SubElement(linear_ring, '{%s}posList' % gml) \n",
    "\n",
    "\n",
    "    result = etree.SubElement(root, '{%s}result' % om)\n",
    "    earth_observation_result = etree.SubElement(result, '{%s}EarthObservationResult' % opt)\n",
    "    cloud_cover_percentage = etree.SubElement(earth_observation_result, '{%s}cloudCoverPercentage' % opt)\n",
    "    \n",
    "    metadata_property = etree.SubElement(root, '{%s}metaDataProperty' % eop)\n",
    "    earth_observation_metadata = etree.SubElement(metadata_property, '{%s}EarthObservationMetaData' % eop)\n",
    "    identifier = etree.SubElement(earth_observation_metadata, '{%s}identifier' % eop)\n",
    "    \n",
    "    begin_position.text = metadata['startdate']\n",
    "    end_position.text = metadata['enddate']\n",
    "    \n",
    "    coords = np.asarray([t[::-1] for t in list(loads(metadata['wkt']).exterior.coords)]).tolist()\n",
    " \n",
    "    pos_list = ''\n",
    "    for elem in coords:\n",
    "        pos_list += ' '.join(str(e) for e in elem) + ' '   \n",
    "\n",
    "    poslist.attrib['count'] = str(len(coords))\n",
    "    poslist.text = pos_list\n",
    "    \n",
    "    \n",
    "    identifier.text = metadata['identifier'] \n",
    "\n",
    "    return etree.tostring(root, pretty_print=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the result WKT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = gdal.Open(output_name + '.tif')\n",
    "ulx, xres, xskew, uly, yskew, yres  = src.GetGeoTransform()\n",
    "\n",
    "max_x = ulx + (src.RasterXSize * xres)\n",
    "min_y = uly + (src.RasterYSize * yres)\n",
    "min_x = ulx \n",
    "max_y = uly\n",
    "\n",
    "source = osr.SpatialReference()\n",
    "source.ImportFromWkt(src.GetProjection())\n",
    "\n",
    "target = osr.SpatialReference()\n",
    "target.ImportFromEPSG(4326)\n",
    "\n",
    "transform = osr.CoordinateTransformation(source, target)\n",
    "\n",
    "result_wkt = box(transform.TransformPoint(min_x, min_y)[0],\n",
    "        transform.TransformPoint(min_x, min_y)[1],\n",
    "        transform.TransformPoint(max_x, max_y)[0],\n",
    "        transform.TransformPoint(max_x, max_y)[1]).wkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the EOP XML metadata file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eop_metadata = dict()\n",
    "\n",
    "eop_metadata['wkt'] = result_wkt\n",
    "eop_metadata['startdate'] = parser.parse(min(dates)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "eop_metadata['enddate'] = parser.parse(max(dates)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "eop_xml = output_name + '.xml'\n",
    "with open(eop_xml, 'wb') as file:\n",
    "    file.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    file.write(eop_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the properties file for the reproducibility notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for properties_file in ['result', 'stage-in']:\n",
    "\n",
    "    if properties_file == 'result':\n",
    "        title = 'Reproducibility notebook used for generating %s' % output_name\n",
    "    else: \n",
    "        title = 'Reproducibility stage-in notebook for Sentinel-1 data for generating %s' % output_name\n",
    "        \n",
    "    with open(properties_file + '.properties', 'wb') as file:\n",
    "        file.write('title=%s\\n' % title)\n",
    "        file.write('date=%s/%s\\n' % (search[0]['startdate'], search[0]['enddate']))\n",
    "        file.write('geometry=%s' % (search[0]['wkt']))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_name + '.tif', 'rb') as f_in, gzip.open(output_name + '.gz', 'wb') as f_out:\n",
    "    shutil.copyfileobj(f_in, f_out)\n",
    "    \n",
    "os.remove(output_name + '.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This work is licenced under a [Attribution-ShareAlike 4.0 International License (CC BY-SA 4.0)](http://creativecommons.org/licenses/by-sa/4.0/) \n",
    "\n",
    "YOU ARE FREE TO:\n",
    "\n",
    "* Share - copy and redistribute the material in any medium or format.\n",
    "* Adapt - remix, transform, and built upon the material for any purpose, even commercially.\n",
    "\n",
    "UNDER THE FOLLOWING TERMS:\n",
    "\n",
    "* Attribution - You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n",
    "* ShareAlike - If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
